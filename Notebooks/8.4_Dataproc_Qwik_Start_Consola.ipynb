{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Laboratorio: Dataproc ‚Äì Qwik Start (Consola)\n",
        "\n",
        "**Google Cloud Dataproc** es un servicio administrado para crear y gestionar clusters de **Apache Spark** y **Apache Hadoop** en la nube. Permite procesar grandes vol√∫menes de datos de forma r√°pida, escalable y a bajo costo, ideal para an√°lisis financieros, auditor√≠as masivas o reportes contables.\n",
        "\n",
        "üí° **Beneficio empresarial**: **Dataproc** permite a las empresas procesar datos financieros masivos, como transacciones o registros contables, en minutos, con la flexibilidad de escalar recursos seg√∫n la demanda y pagar solo por el tiempo de uso, optimizando costos operativos.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataproc](https://cloud.google.com/dataproc/docs).\n",
        "\n",
        "## üìö Concepto clave: ¬øQu√© es Dataproc?\n",
        "\n",
        "| **Servicio** | **Explicaci√≥n para perfiles de negocios** |\n",
        "|--------------|------------------------------------------|\n",
        "| **Google Cloud Dataproc** | Servicio que crea clusters de **Apache Spark** y **Apache Hadoop** en la nube para procesar grandes vol√∫menes de datos r√°pidamente. Permite encender un cluster en minutos, escalarlo (m√°s o menos nodos) y apagarlo cuando no se usa, pagando solo por el tiempo activo. |\n",
        "\n",
        "üí° **Contexto empresarial**: **Dataproc** es como un equipo contable automatizado que puede crecer o reducirse seg√∫n la carga de trabajo, procesando grandes vol√∫menes de transacciones de forma eficiente y econ√≥mica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuraci√≥n previa y requisitos\n",
        "\n",
        "Antes de crear un cluster, aseg√∫rate de que la **API de Dataproc** est√© habilitada y que la cuenta de servicio tenga los permisos necesarios.\n",
        "\n",
        "### Paso 1: Verificar que la API de Dataproc est√© habilitada\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **APIs & Services** > **Library**.\n",
        "2. Busca **Cloud Dataproc API** y √°brela.\n",
        "3. Si aparece **Enable**, haz clic para activarla. Si aparece **Disable**, ya est√° activa.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Habilitar la API permite que **Dataproc** funcione en el proyecto.\n",
        "\n",
        "üí° **Contexto empresarial**: Activar la API es como habilitar un m√≥dulo en un sistema contable para procesar datos financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2: Conceder permisos de almacenamiento al service account\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **IAM & Admin** > **IAM**.\n",
        "2. Busca la cuenta `compute@developer.gserviceaccount.com` y haz clic en ‚úèÔ∏è (editar).\n",
        "3. Haz clic en **+ ADD ANOTHER ROLE** > selecciona **Storage Admin**.\n",
        "4. Haz clic en **Save** üíæ.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El rol **Storage Admin** permite al cluster leer y escribir en **Cloud Storage**, necesario para almacenar logs, archivos temporales y resultados.\n",
        "\n",
        "üí° **Contexto empresarial**: Esto es como otorgar permisos a un equipo contable para acceder a un archivo digital donde se guardan registros financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üü† Tarea 1: Crear un cluster\n",
        "\n",
        "En esta tarea, crear√°s un cluster de **Dataproc** para procesar datos con **Apache Spark**.\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **View all products** > **Dataproc** > **Clusters** > **Create cluster**.\n",
        "2. Selecciona **Cluster on Compute Engine**.\n",
        "3. Configura:\n",
        "\n",
        "| **Campo** | **Valor** |\n",
        "|-----------|-----------|\n",
        "| **Name** | `example-cluster` |\n",
        "| **Region** | `us-central1` |\n",
        "| **Zone** | `us-central1-a` |\n",
        "| **Primary disk type (Master)** | `Standard Persistent Disk` |\n",
        "| **Machine Series (Master)** | `E2` |\n",
        "| **Machine Type (Master)** | `e2-standard-2` |\n",
        "| **Disk size (Master)** | `30 GB` |\n",
        "| **Number of Worker Nodes** | `2` |\n",
        "| **Machine Type (Workers)** | `e2-standard-2` |\n",
        "| **Disk size (Workers)** | `30 GB` |\n",
        "| **Internal IP only** | ‚ùå Desmarcar |\n",
        "\n",
        "4. Haz clic en **Create**.\n",
        "\n",
        "**Resultado esperado**:\n",
        "- El estado del cluster pasar√° de **Provisioning** a **Running**.\n",
        "\n",
        "**Conceptos clave**:\n",
        "- **Master node**: Coordina las tareas del cluster.\n",
        "- **Worker nodes**: Ejecutan las tareas de procesamiento de **Spark** o **Hadoop**.\n",
        "- **E2**: Familia de m√°quinas virtuales balanceadas en precio y rendimiento.\n",
        "- **Regi√≥n/Zona**: Define la ubicaci√≥n f√≠sica de los recursos, impactando la latencia y el cumplimiento normativo.\n",
        "\n",
        "üí° **Contexto empresarial**: Crear un cluster es como montar un equipo contable que procesa grandes vol√∫menes de datos financieros, con un l√≠der (master) y trabajadores (workers) que dividen las tareas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üü† Tarea 2: Enviar un job Spark\n",
        "\n",
        "En esta tarea, enviar√°s un job de **Apache Spark** para calcular el valor de œÄ usando el m√©todo Monte Carlo.\n",
        "\n",
        "1. En la barra lateral de **Dataproc**, selecciona **Jobs** > **Submit job**.\n",
        "2. Configura:\n",
        "\n",
        "| **Campo** | **Valor** |\n",
        "|-----------|-----------|\n",
        "| **Region** | Misma que el cluster (`us-central1`) |\n",
        "| **Cluster** | `example-cluster` |\n",
        "| **Job type** | `Spark` |\n",
        "| **Main class or jar** | `org.apache.spark.examples.SparkPi` |\n",
        "| **Jar files** | `file:///usr/lib/spark/examples/jars/spark-examples.jar` |\n",
        "| **Arguments** | `1000` |\n",
        "\n",
        "3. Haz clic en **Submit** ‚ñ∂Ô∏è.\n",
        "\n",
        "**Desglose de par√°metros**:\n",
        "| **Elemento** | **¬øQu√© es?** | **Uso pr√°ctico** |\n",
        "|--------------|--------------|------------------|\n",
        "| `org.apache.spark.examples.SparkPi` | Clase Java incluida con Spark que calcula œÄ. | Define la l√≥gica ejecutada. |\n",
        "| `file:///usr/lib/spark/.../spark-examples.jar` | Ruta local en cada nodo hacia el JAR de ejemplos. | Contiene la clase anterior. |\n",
        "| `Arguments 1000` | N√∫mero de puntos aleatorios (iteraciones). | M√°s iteraciones = mayor precisi√≥n, m√°s carga de c√≥mputo. |\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El job **SparkPi** utiliza el m√©todo Monte Carlo, generando 1000 puntos aleatorios en un cuadrado que circunscribe un c√≠rculo de radio 1. La proporci√≥n de puntos dentro del c√≠rculo aproxima œÄ/4, y **Spark** paraleliza el c√°lculo entre los workers.\n",
        "- El estado del job pasar√° de **Running** a **Succeeded** ‚úîÔ∏è.\n",
        "\n",
        "üí° **Contexto empresarial**: Enviar un job es como asignar una tarea a un equipo contable para calcular un indicador financiero, distribuyendo el trabajo para mayor eficiencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üü† Tarea 3: Ver la salida del job\n",
        "\n",
        "1. En la lista de jobs, haz clic en el **Job ID**.\n",
        "2. Activa **LINE WRAP ON** o desplaza a la derecha.\n",
        "3. Ver√°s una salida similar a:\n",
        "   ```\n",
        "   Pi is roughly 3.141592...\n",
        "   ```\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- La salida muestra el valor aproximado de œÄ calculado por el job.\n",
        "\n",
        "üí° **Contexto empresarial**: Ver la salida es como revisar el resultado de un c√°lculo financiero, como un indicador de rentabilidad, generado por el equipo contable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üü† Tarea 4: Aumentar la cantidad de workers\n",
        "\n",
        "En esta tarea, escalar√°s el cluster para aumentar su capacidad de procesamiento.\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **Dataproc** > **Clusters** > selecciona `example-cluster`.\n",
        "2. En la pesta√±a **Configuration**, haz clic en **Edit** ‚úèÔ∏è.\n",
        "3. Cambia **Worker nodes** de `2` a `4` > **Save**.\n",
        "4. El cluster se actualizar√°, a√±adiendo 2 VMs adicionales.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Escalar el n√∫mero de workers aumenta el paralelismo, reduciendo el tiempo de procesamiento para trabajos con grandes vol√∫menes de datos.\n",
        "- Para probar el impacto, repite el job de la **Tarea 2** con los mismos par√°metros.\n",
        "\n",
        "üí° **Contexto empresarial**: Escalar el cluster es como contratar m√°s contadores durante un cierre fiscal para procesar datos m√°s r√°pido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üü† Tarea 5: Preguntas de repaso\n",
        "\n",
        "| **Pregunta** | **Respuesta** |\n",
        "|--------------|---------------|\n",
        "| ¬øQu√© tipo de job se env√≠a en este lab? | ‚úÖ Spark |\n",
        "| Dataproc ayuda a procesar, transformar y entender grandes vol√∫menes de datos. | ‚úÖ Verdadero |\n",
        "\n",
        "## üöÄ Resumen\n",
        "\n",
        "| **Concepto** | **Explicaci√≥n contable simplificada** |\n",
        "|--------------|--------------------------------------|\n",
        "| **Dataproc** | Sistema que crea equipos contables automatizados para procesar grandes vol√∫menes de datos financieros. |\n",
        "| **Cluster** | Grupo de m√°quinas (master y workers) que trabajan juntas para procesar datos. |\n",
        "| **Spark** | Herramienta que distribuye tareas de c√°lculo, como res√∫menes financieros, entre m√∫ltiples m√°quinas. |\n",
        "| **Escalabilidad** | Capacidad de a√±adir o quitar contadores (workers) seg√∫n la carga de trabajo, optimizando costos. |\n",
        "\n",
        "üí° **Conclusi√≥n empresarial**: **Dataproc** permite a las empresas procesar grandes vol√∫menes de datos financieros, como transacciones o auditor√≠as, de forma r√°pida y escalable. La capacidad de encender, escalar y apagar clusters reduce costos y asegura cumplimiento normativo, ideal para cierres fiscales o an√°lisis masivos.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}