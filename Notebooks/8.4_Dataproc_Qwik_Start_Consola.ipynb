{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧪 Laboratorio: Dataproc – Qwik Start (Consola)\n",
        "\n",
        "**Google Cloud Dataproc** es un servicio administrado para crear y gestionar clusters de **Apache Spark** y **Apache Hadoop** en la nube. Permite procesar grandes volúmenes de datos de forma rápida, escalable y a bajo costo, ideal para análisis financieros, auditorías masivas o reportes contables.\n",
        "\n",
        "💡 **Beneficio empresarial**: **Dataproc** permite a las empresas procesar datos financieros masivos, como transacciones o registros contables, en minutos, con la flexibilidad de escalar recursos según la demanda y pagar solo por el tiempo de uso, optimizando costos operativos.\n",
        "\n",
        "Para más información, consulta la [Documentación de Google Cloud Dataproc](https://cloud.google.com/dataproc/docs).\n",
        "\n",
        "## 📚 Concepto clave: ¿Qué es Dataproc?\n",
        "\n",
        "| **Servicio** | **Explicación para perfiles de negocios** |\n",
        "|--------------|------------------------------------------|\n",
        "| **Google Cloud Dataproc** | Servicio que crea clusters de **Apache Spark** y **Apache Hadoop** en la nube para procesar grandes volúmenes de datos rápidamente. Permite encender un cluster en minutos, escalarlo (más o menos nodos) y apagarlo cuando no se usa, pagando solo por el tiempo activo. |\n",
        "\n",
        "💡 **Contexto empresarial**: **Dataproc** es como un equipo contable automatizado que puede crecer o reducirse según la carga de trabajo, procesando grandes volúmenes de transacciones de forma eficiente y económica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Configuración previa y requisitos\n",
        "\n",
        "Antes de crear un cluster, asegúrate de que la **API de Dataproc** esté habilitada y que la cuenta de servicio tenga los permisos necesarios.\n",
        "\n",
        "### Paso 1: Verificar que la API de Dataproc esté habilitada\n",
        "\n",
        "1. Ve a **Navigation menu** ☰ > **APIs & Services** > **Library**.\n",
        "2. Busca **Cloud Dataproc API** y ábrela.\n",
        "3. Si aparece **Enable**, haz clic para activarla. Si aparece **Disable**, ya está activa.\n",
        "\n",
        "**Explicación**:\n",
        "- Habilitar la API permite que **Dataproc** funcione en el proyecto.\n",
        "\n",
        "💡 **Contexto empresarial**: Activar la API es como habilitar un módulo en un sistema contable para procesar datos financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2: Conceder permisos de almacenamiento al service account\n",
        "\n",
        "1. Ve a **Navigation menu** ☰ > **IAM & Admin** > **IAM**.\n",
        "2. Busca la cuenta `compute@developer.gserviceaccount.com` y haz clic en ✏️ (editar).\n",
        "3. Haz clic en **+ ADD ANOTHER ROLE** > selecciona **Storage Admin**.\n",
        "4. Haz clic en **Save** 💾.\n",
        "\n",
        "**Explicación**:\n",
        "- El rol **Storage Admin** permite al cluster leer y escribir en **Cloud Storage**, necesario para almacenar logs, archivos temporales y resultados.\n",
        "\n",
        "💡 **Contexto empresarial**: Esto es como otorgar permisos a un equipo contable para acceder a un archivo digital donde se guardan registros financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🟠 Tarea 1: Crear un cluster\n",
        "\n",
        "En esta tarea, crearás un cluster de **Dataproc** para procesar datos con **Apache Spark**.\n",
        "\n",
        "1. Ve a **Navigation menu** ☰ > **View all products** > **Dataproc** > **Clusters** > **Create cluster**.\n",
        "2. Selecciona **Cluster on Compute Engine**.\n",
        "3. Configura:\n",
        "\n",
        "| **Campo** | **Valor** |\n",
        "|-----------|-----------|\n",
        "| **Name** | `example-cluster` |\n",
        "| **Region** | `us-central1` |\n",
        "| **Zone** | `us-central1-a` |\n",
        "| **Primary disk type (Master)** | `Standard Persistent Disk` |\n",
        "| **Machine Series (Master)** | `E2` |\n",
        "| **Machine Type (Master)** | `e2-standard-2` |\n",
        "| **Disk size (Master)** | `30 GB` |\n",
        "| **Number of Worker Nodes** | `2` |\n",
        "| **Machine Type (Workers)** | `e2-standard-2` |\n",
        "| **Disk size (Workers)** | `30 GB` |\n",
        "| **Internal IP only** | ❌ Desmarcar |\n",
        "\n",
        "4. Haz clic en **Create**.\n",
        "\n",
        "**Resultado esperado**:\n",
        "- El estado del cluster pasará de **Provisioning** a **Running**.\n",
        "\n",
        "**Conceptos clave**:\n",
        "- **Master node**: Coordina las tareas del cluster.\n",
        "- **Worker nodes**: Ejecutan las tareas de procesamiento de **Spark** o **Hadoop**.\n",
        "- **E2**: Familia de máquinas virtuales balanceadas en precio y rendimiento.\n",
        "- **Región/Zona**: Define la ubicación física de los recursos, impactando la latencia y el cumplimiento normativo.\n",
        "\n",
        "💡 **Contexto empresarial**: Crear un cluster es como montar un equipo contable que procesa grandes volúmenes de datos financieros, con un líder (master) y trabajadores (workers) que dividen las tareas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🟠 Tarea 2: Enviar un job Spark\n",
        "\n",
        "En esta tarea, enviarás un job de **Apache Spark** para calcular el valor de π usando el método Monte Carlo.\n",
        "\n",
        "1. En la barra lateral de **Dataproc**, selecciona **Jobs** > **Submit job**.\n",
        "2. Configura:\n",
        "\n",
        "| **Campo** | **Valor** |\n",
        "|-----------|-----------|\n",
        "| **Region** | Misma que el cluster (`us-central1`) |\n",
        "| **Cluster** | `example-cluster` |\n",
        "| **Job type** | `Spark` |\n",
        "| **Main class or jar** | `org.apache.spark.examples.SparkPi` |\n",
        "| **Jar files** | `file:///usr/lib/spark/examples/jars/spark-examples.jar` |\n",
        "| **Arguments** | `1000` |\n",
        "\n",
        "3. Haz clic en **Submit** ▶️.\n",
        "\n",
        "**Desglose de parámetros**:\n",
        "| **Elemento** | **¿Qué es?** | **Uso práctico** |\n",
        "|--------------|--------------|------------------|\n",
        "| `org.apache.spark.examples.SparkPi` | Clase Java incluida con Spark que calcula π. | Define la lógica ejecutada. |\n",
        "| `file:///usr/lib/spark/.../spark-examples.jar` | Ruta local en cada nodo hacia el JAR de ejemplos. | Contiene la clase anterior. |\n",
        "| `Arguments 1000` | Número de puntos aleatorios (iteraciones). | Más iteraciones = mayor precisión, más carga de cómputo. |\n",
        "\n",
        "**Explicación**:\n",
        "- El job **SparkPi** utiliza el método Monte Carlo, generando 1000 puntos aleatorios en un cuadrado que circunscribe un círculo de radio 1. La proporción de puntos dentro del círculo aproxima π/4, y **Spark** paraleliza el cálculo entre los workers.\n",
        "- El estado del job pasará de **Running** a **Succeeded** ✔️.\n",
        "\n",
        "💡 **Contexto empresarial**: Enviar un job es como asignar una tarea a un equipo contable para calcular un indicador financiero, distribuyendo el trabajo para mayor eficiencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🟠 Tarea 3: Ver la salida del job\n",
        "\n",
        "1. En la lista de jobs, haz clic en el **Job ID**.\n",
        "2. Activa **LINE WRAP ON** o desplaza a la derecha.\n",
        "3. Verás una salida similar a:\n",
        "   ```\n",
        "   Pi is roughly 3.141592...\n",
        "   ```\n",
        "\n",
        "**Explicación**:\n",
        "- La salida muestra el valor aproximado de π calculado por el job.\n",
        "\n",
        "💡 **Contexto empresarial**: Ver la salida es como revisar el resultado de un cálculo financiero, como un indicador de rentabilidad, generado por el equipo contable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🟠 Tarea 4: Aumentar la cantidad de workers\n",
        "\n",
        "En esta tarea, escalarás el cluster para aumentar su capacidad de procesamiento.\n",
        "\n",
        "1. Ve a **Navigation menu** ☰ > **Dataproc** > **Clusters** > selecciona `example-cluster`.\n",
        "2. En la pestaña **Configuration**, haz clic en **Edit** ✏️.\n",
        "3. Cambia **Worker nodes** de `2` a `4` > **Save**.\n",
        "4. El cluster se actualizará, añadiendo 2 VMs adicionales.\n",
        "\n",
        "**Explicación**:\n",
        "- Escalar el número de workers aumenta el paralelismo, reduciendo el tiempo de procesamiento para trabajos con grandes volúmenes de datos.\n",
        "- Para probar el impacto, repite el job de la **Tarea 2** con los mismos parámetros.\n",
        "\n",
        "💡 **Contexto empresarial**: Escalar el cluster es como contratar más contadores durante un cierre fiscal para procesar datos más rápido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🟠 Tarea 5: Preguntas de repaso\n",
        "\n",
        "| **Pregunta** | **Respuesta** |\n",
        "|--------------|---------------|\n",
        "| ¿Qué tipo de job se envía en este lab? | ✅ Spark |\n",
        "| Dataproc ayuda a procesar, transformar y entender grandes volúmenes de datos. | ✅ Verdadero |\n",
        "\n",
        "## 🚀 Resumen\n",
        "\n",
        "| **Concepto** | **Explicación contable simplificada** |\n",
        "|--------------|--------------------------------------|\n",
        "| **Dataproc** | Sistema que crea equipos contables automatizados para procesar grandes volúmenes de datos financieros. |\n",
        "| **Cluster** | Grupo de máquinas (master y workers) que trabajan juntas para procesar datos. |\n",
        "| **Spark** | Herramienta que distribuye tareas de cálculo, como resúmenes financieros, entre múltiples máquinas. |\n",
        "| **Escalabilidad** | Capacidad de añadir o quitar contadores (workers) según la carga de trabajo, optimizando costos. |\n",
        "\n",
        "💡 **Conclusión empresarial**: **Dataproc** permite a las empresas procesar grandes volúmenes de datos financieros, como transacciones o auditorías, de forma rápida y escalable. La capacidad de encender, escalar y apagar clusters reduce costos y asegura cumplimiento normativo, ideal para cierres fiscales o análisis masivos.\n",
        "\n",
        "Para más información, consulta la [Documentación de Google Cloud Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}