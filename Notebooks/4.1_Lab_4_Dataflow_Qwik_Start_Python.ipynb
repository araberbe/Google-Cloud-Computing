{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìå Laboratorio: Dataflow Qwik Start - Python\n",
        "\n",
        "## üåê Descripci√≥n General\n",
        "**Apache Beam SDK** es un modelo de programaci√≥n de c√≥digo abierto para crear canalizaciones (pipelines) de datos. En **Google Cloud**, puedes definir pipelines con **Apache Beam** y ejecutarlos en **Dataflow**. Este laboratorio te gu√≠a para configurar un entorno de desarrollo en Python, instalar **Apache Beam**, y ejecutar un pipeline remotamente.\n",
        "\n",
        "üí° **Contexto empresarial**: Similar a configurar un sistema automatizado que procesa transacciones financieras, como facturas o registros de ventas, y genera reportes consolidados en tiempo r√©cord.\n",
        "\n",
        "### Objetivos del Laboratorio\n",
        "- Crear un **bucket** en **Cloud Storage** para almacenar resultados.\n",
        "- Instalar el **Apache Beam SDK** para Python.\n",
        "- Ejecutar un pipeline de **Dataflow** remotamente.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Dataflow](https://cloud.google.com/dataflow/docs) y [Documentaci√≥n de Apache Beam](https://beam.apache.org/documentation/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Requisitos Previos y Configuraci√≥n Inicial\n",
        "\n",
        "### Antes de Comenzar\n",
        "- Usa un entorno real de **Google Cloud** con credenciales temporales.\n",
        "- No pauses el laboratorio una vez iniciado.\n",
        "- Usa la cuenta estudiantil proporcionada, no una cuenta personal.\n",
        "- Abre el navegador en modo inc√≥gnito para evitar conflictos.\n",
        "\n",
        "### Iniciar el Laboratorio\n",
        "1. Haz clic en **Start Lab** en la plataforma del laboratorio.\n",
        "2. Copia las credenciales temporales (**usuario** y **contrase√±a**).\n",
        "3. Haz clic en **Open Google Cloud Console**.\n",
        "4. Inicia sesi√≥n, acepta los t√©rminos y condiciones, y no configures recuperaci√≥n ni autenticaci√≥n en dos pasos.\n",
        "\n",
        "### Activar Cloud Shell\n",
        "**Cloud Shell** es una m√°quina virtual con herramientas preinstaladas para desarrolladores.\n",
        "\n",
        "#### Pasos\n",
        "1. Haz clic en **Activate Cloud Shell** en la consola.\n",
        "2. Acepta los permisos.\n",
        "\n",
        "#### Comandos Opcionales\n",
        "Verifica la cuenta y el proyecto activos, y configura la regi√≥n (reemplaza `REGION` con un valor espec√≠fico, ej. `us-central1`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud auth list\ngcloud config list project\ngcloud config set compute/region REGION",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Confirma el proyecto y la regi√≥n, como verificar el libro contable y la sucursal antes de procesar transacciones.\n",
        "\n",
        "### Verificar la API de Dataflow\n",
        "1. Busca **Dataflow API** en la barra de b√∫squeda de la consola.\n",
        "2. Haz clic en **Manage**.\n",
        "3. Deshabilita y vuelve a habilitar la API.\n",
        "\n",
        "üí° **Prop√≥sito**: Asegura que el servicio est√© activo, como validar un sistema contable antes de usarlo.\n",
        "\n",
        "üí° **Beneficio empresarial**: Garantiza que el pipeline funcione sin errores, optimizando procesos financieros.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Cloud Shell](https://cloud.google.com/shell/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Tarea 1: Crear un Bucket de Cloud Storage\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Cloud Storage > Buckets** en el men√∫ de navegaci√≥n.\n",
        "2. Haz clic en **Create bucket**.\n",
        "3. Configura:\n",
        "   - **Name**: Un nombre √∫nico (ej. `nombre-bucket`).\n",
        "   - **Location type**: Multi-region.\n",
        "   - **Location**: `us`.\n",
        "4. Confirma la prevenci√≥n de acceso p√∫blico.\n",
        "\n",
        "### Comando Equivalente\n",
        "Crea el bucket desde **Cloud Shell** (reemplaza `nombre-bucket` con un nombre √∫nico):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gsutil mb gs://nombre-bucket/",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: El bucket almacena resultados del pipeline, como una carpeta temporal para documentos financieros antes de procesarlos.\n",
        "\n",
        "üí° **Beneficio empresarial**: Proporciona almacenamiento escalable y seguro para datos intermedios.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Tarea 2: Instalar Apache Beam SDK para Python\n",
        "\n",
        "### Paso 1: Ejecutar Python 3.9 desde Docker\n",
        "Inicia un contenedor con Python 3.9:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID python:3.9 /bin/bash",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Crea un entorno aislado con Python 3.9, como configurar un software contable espec√≠fico para un proyecto.\n",
        "\n",
        "### Paso 2: Instalar Apache Beam\n",
        "Instala el SDK con soporte para Google Cloud:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "pip install 'apache-beam[gcp]'==2.42.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Prepara el entorno para desarrollar pipelines, como instalar un m√≥dulo de an√°lisis financiero.\n",
        "\n",
        "### Paso 3: Ejecutar Ejemplo Localmente\n",
        "Prueba el ejemplo `wordcount` (reemplaza `OUTPUT_FILE` con un nombre, ej. `output.txt`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "python -m apache_beam.examples.wordcount --output OUTPUT_FILE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifica los resultados:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ls\ncat OUTPUT_FILE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Ejecuta un pipeline localmente para contar palabras, como probar un sistema contable con datos de muestra.\n",
        "\n",
        "üí° **Analog√≠a contable**: Similar a contar transacciones por categor√≠a (ej. ingresos por producto) antes de procesarlas a gran escala.\n",
        "\n",
        "üí° **Beneficio empresarial**: Valida el entorno antes de ejecutar pipelines complejos, asegurando precisi√≥n en an√°lisis financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Tarea 3: Ejecutar la Canalizaci√≥n de Dataflow Remotamente\n",
        "\n",
        "### Paso 1: Definir Variable BUCKET\n",
        "Configura la variable del bucket (reemplaza `nombre-del-bucket` con el nombre creado):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "BUCKET=gs://nombre-del-bucket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Define la ubicaci√≥n del bucket, como especificar la carpeta donde se guardar√°n los reportes financieros.\n",
        "\n",
        "### Paso 2: Ejecutar el Pipeline\n",
        "Ejecuta el pipeline `wordcount` en **Dataflow** (reemplaza `REGION` con un valor, ej. `us-central1`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "python -m apache_beam.examples.wordcount --project $DEVSHELL_PROJECT_ID \\\n  --runner DataflowRunner \\\n  --staging_location $BUCKET/staging \\\n  --temp_location $BUCKET/temp \\\n  --output $BUCKET/results/output \\\n  --region REGION",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Par√°metro | Funci√≥n |\n",
        "|-----------|---------|\n",
        "| `--project` | ID del proyecto activo. |\n",
        "| `--runner DataflowRunner` | Ejecuta el pipeline en **Dataflow**. |\n",
        "| `--staging_location` | Carpeta temporal para archivos del entorno. |\n",
        "| `--temp_location` | Almacena archivos temporales del proceso. |\n",
        "| `--output` | Directorio para resultados. |\n",
        "| `--region` | Regi√≥n de ejecuci√≥n. |\n",
        "\n",
        "üí° **Prop√≥sito**: Ejecuta el pipeline remotamente, procesando datos a gran escala, como consolidar transacciones de m√∫ltiples sucursales.\n",
        "\n",
        "üí° **Analog√≠a contable**: Como configurar un sistema autom√°tico para procesar y resumir miles de registros financieros en la nube.\n",
        "\n",
        "üí° **Beneficio empresarial**: Automatiza el an√°lisis de datos, reduciendo tiempo y costos en reportes financieros.\n",
        "\n",
        "Espera el mensaje: **‚ÄúJOB_MESSAGE_DETAILED: Workers have started successfully.‚Äù**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Tarea 4: Verificar el √âxito del Job\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Dataflow** en el men√∫ de navegaci√≥n.\n",
        "2. Busca el job `wordcount`.\n",
        "3. Verifica que el estado sea **Succeeded**.\n",
        "4. Ve a **Cloud Storage > Buckets**.\n",
        "5. Abre el bucket y revisa las carpetas `results` y `staging`.\n",
        "6. Abre un archivo en `results` para ver el conteo de palabras.\n",
        "\n",
        "üí° **Prop√≥sito**: Confirma que el pipeline proces√≥ los datos correctamente, como revisar un informe financiero para validar su precisi√≥n.\n",
        "\n",
        "üí° **Beneficio empresarial**: Garantiza resultados confiables para an√°lisis financieros o auditor√≠as."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Tarea 5: Pregunta de Verificaci√≥n\n",
        "\n",
        "**Pregunta**: La ubicaci√≥n temporal de **Dataflow** (`temp_location`) debe ser una URL v√°lida de **Cloud Storage**.\n",
        "- ‚úÖ **Verdadero**\n",
        "\n",
        "üí° **Contexto empresarial**: Esto asegura que los datos intermedios se almacenen correctamente, como guardar documentos temporales en un archivador seguro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Cuadro Resumen Final\n",
        "\n",
        "| Elemento | Descripci√≥n |\n",
        "|----------|-------------|\n",
        "| **Producto principal** | Dataflow + Apache Beam |\n",
        "| **Lenguaje usado** | Python |\n",
        "| **Objetivo** | Crear y ejecutar un pipeline de procesamiento de datos |\n",
        "| **Almacenamiento** | Google Cloud Storage (bucket) |\n",
        "| **Entorno** | Cloud Shell + contenedor Docker con Python 3.9 |\n",
        "| **Herramientas** | gcloud, docker, pip, Apache Beam SDK |\n",
        "| **Ejecuci√≥n** | Local (DirectRunner) y Remota (DataflowRunner) |\n",
        "| **Resultado** | Archivo con conteo de palabras |\n",
        "\n",
        "üí° **Conclusi√≥n empresarial**: Este laboratorio automatiza el procesamiento de datos, como consolidar transacciones financieras, optimizando eficiencia y costos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Conclusi√≥n\n",
        "\n",
        "Este laboratorio te permiti√≥:\n",
        "1. Crear un **bucket** en **Cloud Storage**.\n",
        "2. Instalar **Apache Beam SDK** en un entorno Docker.\n",
        "3. Ejecutar un pipeline de **Dataflow** remotamente.\n",
        "4. Verificar los resultados en **Cloud Storage**.\n",
        "\n",
        "üí° **Beneficio empresarial**: Automatiza procesos ETL para datos financieros, como procesar registros de ventas en tiempo real, reduciendo errores y acelerando reportes.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Dataflow](https://cloud.google.com/dataflow/docs), [Documentaci√≥n de Apache Beam](https://beam.apache.org/documentation/), y [Documentaci√≥n de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}