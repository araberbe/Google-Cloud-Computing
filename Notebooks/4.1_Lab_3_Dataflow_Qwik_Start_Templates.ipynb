{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📌 Laboratorio: Dataflow Qwik Start - Templates\n",
        "\n",
        "## 🎯 Objetivo General\n",
        "Este laboratorio muestra cómo usar una plantilla predefinida de **Dataflow** para recibir datos en tiempo real desde **Pub/Sub**, procesarlos automáticamente y almacenarlos en una tabla de **BigQuery**. \n",
        "\n",
        "💡 **Contexto empresarial**: Similar a una caja registradora digital que registra transacciones en tiempo real y las carga automáticamente a un libro contable digital, optimizando procesos financieros sin intervención manual.\n",
        "\n",
        "### Objetivos del Laboratorio\n",
        "- Activar la **Dataflow API**.\n",
        "- Crear un **dataset** y una tabla en **BigQuery**, y un **bucket** en **Cloud Storage**.\n",
        "- Ejecutar un pipeline de **Dataflow** usando una plantilla.\n",
        "- Consultar los datos procesados en **BigQuery**.\n",
        "\n",
        "Para más información, consulta la [Documentación de Dataflow](https://cloud.google.com/dataflow/docs), [Documentación de BigQuery](https://cloud.google.com/bigquery/docs), y [Documentación de Pub/Sub](https://cloud.google.com/pubsub/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Tarea 1: Activar y Reiniciar la API de Dataflow\n",
        "\n",
        "### Pasos\n",
        "1. En la consola de **Google Cloud**, busca **Dataflow API** en la barra superior.\n",
        "2. Haz clic en **Dataflow API** > **Manage**.\n",
        "3. Selecciona **Disable API** y confirma.\n",
        "4. Luego, haz clic en **Enable** para reactivar la API.\n",
        "\n",
        "💡 **Propósito**: Reiniciar la API asegura que el servicio esté correctamente configurado, como verificar que un sistema contable esté listo antes de procesar transacciones.\n",
        "\n",
        "💡 **Beneficio empresarial**: Garantiza que el pipeline funcione sin errores, similar a validar un software contable antes de usarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Tarea 2: Crear Dataset, Tabla y Bucket\n",
        "\n",
        "### Crear un Dataset en BigQuery\n",
        "Crea un dataset llamado `taxirides` para almacenar datos de viajes:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "bq mk taxirides",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: El dataset es como un libro contable digital que organiza los registros de viajes.\n",
        "\n",
        "💡 **Analogía contable**: Similar a crear un nuevo archivo para registrar transacciones financieras.\n",
        "\n",
        "### Crear una Tabla en BigQuery\n",
        "Crea una tabla `realtime` en el dataset `taxirides` con un esquema específico:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "bq mk \\\n  --time_partitioning_field timestamp \\\n  --schema ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer \\\n  -t taxirides.realtime",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Parte del Comando | Qué Hace (Analogía Contable) |\n",
        "|-------------------|-----------------------------|\n",
        "| `--time_partitioning_field timestamp` | Divide los datos por fecha y hora, como organizar un libro contable por días. |\n",
        "| `--schema ...` | Define las columnas (ej. `ride_id`, `timestamp`), como las columnas de un registro financiero. |\n",
        "| `-t taxirides.realtime` | Especifica la tabla `realtime` en el dataset `taxirides`. |\n",
        "\n",
        "💡 **Propósito**: Define la estructura para almacenar datos de viajes, como configurar un libro diario con columnas para transacciones.\n",
        "\n",
        "💡 **Beneficio empresarial**: Permite consultas rápidas y organizadas, ideal para reportes financieros o auditorías.\n",
        "\n",
        "### Crear un Bucket en Cloud Storage\n",
        "1. Define el nombre del bucket (usa el **Project ID** o un nombre único):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "export BUCKET_NAME=\"NombreDelBucket\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Crea el bucket:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gsutil mb gs://$BUCKET_NAME/",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: El bucket actúa como una carpeta temporal para archivos intermedios, como un archivador para documentos financieros antes de registrarlos.\n",
        "\n",
        "💡 **Beneficio empresarial**: Proporciona almacenamiento escalable y seguro para datos en tránsito.\n",
        "\n",
        "Para más información, consulta la [Documentación de BigQuery](https://cloud.google.com/bigquery/docs) y [Documentación de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🖱️ Tarea 3: Alternativa Gráfica (Opcional)\n",
        "\n",
        "Si prefieres la interfaz web, puedes crear el dataset, la tabla y el bucket desde la consola de **Google Cloud**:\n",
        "1. **BigQuery**: Ve a **BigQuery > Create Dataset**, configura `taxirides`, y crea la tabla `realtime` con el esquema especificado.\n",
        "2. **Cloud Storage**: Ve a **Cloud Storage > Buckets > Create**, especifica el nombre del bucket.\n",
        "\n",
        "💡 **Nota**: Omití esta tarea si realizaste la Tarea 2 con comandos en **Cloud Shell**.\n",
        "\n",
        "💡 **Contexto empresarial**: Usar la interfaz web es como configurar un sistema contable manualmente, mientras que los comandos automatizan el proceso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Tarea 4: Ejecutar el Pipeline con Dataflow\n",
        "\n",
        "Ejecuta un pipeline usando una plantilla predefinida para conectar **Pub/Sub** a **BigQuery** (reemplaza `Region`, `Bucket Name`, y `ProjectID` con valores específicos, ej. `us-central1`, `my-project-bucket`, `my-project-id`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataflow jobs run iotflow \\\n  --gcs-location gs://dataflow-templates-Region/latest/PubSub_to_BigQuery \\\n  --region Region \\\n  --worker-machine-type e2-medium \\\n  --staging-location gs://Bucket_Name/temp \\\n  --parameters inputTopic=projects/pubsub-public-data/topics/taxirides-realtime,outputTableSpec=ProjectID:taxirides.realtime",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Elemento | Descripción |\n",
        "|----------|-------------|\n",
        "| `gcloud dataflow jobs run` | Ejecuta un trabajo en **Dataflow**. |\n",
        "| `iotflow` | Nombre del trabajo. |\n",
        "| `--gcs-location` | Ruta a la plantilla **PubSub_to_BigQuery**. |\n",
        "| `--region` | Región donde se ejecuta el pipeline. |\n",
        "| `--worker-machine-type` | Tipo de máquina para el procesamiento. |\n",
        "| `--staging-location` | Bucket para archivos temporales. |\n",
        "| `--parameters` | Define la fuente (**Pub/Sub**) y destino (**BigQuery**). |\n",
        "\n",
        "💡 **Propósito**: Configura un flujo automatizado que procesa datos en tiempo real desde **Pub/Sub** y los carga en **BigQuery**.\n",
        "\n",
        "💡 **Analogía contable**: Como un sistema que registra transacciones de una caja registradora en tiempo real y las carga directamente al libro contable.\n",
        "\n",
        "💡 **Beneficio empresarial**: Automatiza la captura y procesamiento de datos financieros, reduciendo errores y acelerando reportes.\n",
        "\n",
        "Para monitorear, ve a **Google Cloud Console > Dataflow > Jobs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Tarea 5: Consultar los Datos en BigQuery\n",
        "\n",
        "Consulta los datos cargados en la tabla `realtime`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "SELECT * FROM `ProjectID.taxirides.realtime` LIMIT 1000",
      "language": "sql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: Recupera los últimos 1000 registros de viajes, como revisar los últimos comprobantes financieros en un sistema contable.\n",
        "\n",
        "💡 **Beneficio empresarial**: Permite analizar datos en tiempo real para reportes financieros o auditorías rápidas.\n",
        "\n",
        "Para más información, consulta la [Documentación de BigQuery](https://cloud.google.com/bigquery/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📘 Tarea 6: Evaluación Final\n",
        "\n",
        "1. **¿Dataflow permite procesamiento por lotes?**\n",
        "   - ✅ **Verdadero** — Soporta **batch** y **streaming**.\n",
        "\n",
        "2. **¿Qué plantilla se usó para ejecutar el pipeline?**\n",
        "   - ✅ **Pub/Sub to BigQuery**\n",
        "\n",
        "💡 **Contexto empresarial**: Estas preguntas refuerzan la comprensión de **Dataflow** como una herramienta para automatizar procesos contables en tiempo real o por lotes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Cuadro Resumen Final\n",
        "\n",
        "| Elemento | Descripción |\n",
        "|----------|-------------|\n",
        "| **Dataflow API** | Servicio para automatizar procesamiento ETL en tiempo real o por lotes. |\n",
        "| **BigQuery dataset y tabla** | Estructura para guardar y consultar datos, como libros contables digitales. |\n",
        "| **Cloud Storage bucket** | Almacenamiento temporal para archivos en tránsito. |\n",
        "| **Pub/Sub** | Sistema de mensajería en tiempo real, como una caja registradora. |\n",
        "| **Pipeline** | Flujo automatizado que extrae, transforma y carga datos en **BigQuery**. |\n",
        "| **Comando bq mk** | Crea datasets y tablas, definiendo su estructura. |\n",
        "| **Plantilla Dataflow** | **Pub/Sub to BigQuery** para datos en tiempo real. |\n",
        "| **Consulta SQL** | `SELECT * FROM taxirides.realtime LIMIT 1000` para revisar registros. |\n",
        "\n",
        "💡 **Conclusión empresarial**: Este laboratorio automatiza la captura y análisis de datos en tiempo real, ideal para procesar transacciones financieras y generar reportes instantáneos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Conclusión\n",
        "\n",
        "Este laboratorio te permitió:\n",
        "1. Activar la **Dataflow API**.\n",
        "2. Crear un **dataset** y tabla en **BigQuery**, y un **bucket** en **Cloud Storage**.\n",
        "3. Ejecutar un pipeline de **Dataflow** con la plantilla **Pub/Sub to BigQuery**.\n",
        "4. Consultar datos procesados en **BigQuery**.\n",
        "\n",
        "💡 **Beneficio empresarial**: Automatiza procesos ETL para datos financieros, como registrar transacciones en tiempo real, optimizando eficiencia y reduciendo errores.\n",
        "\n",
        "Para más información, consulta la [Documentación de Dataflow](https://cloud.google.com/dataflow/docs), [Documentación de BigQuery](https://cloud.google.com/bigquery/docs), [Documentación de Pub/Sub](https://cloud.google.com/pubsub/docs), y [Documentación de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}