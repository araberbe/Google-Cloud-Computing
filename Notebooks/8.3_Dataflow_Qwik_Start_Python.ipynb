{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Laboratorio: Inicio R√°pido con Dataflow en Python\n",
        "\n",
        "**Apache Beam** es un modelo de programaci√≥n de c√≥digo abierto para construir pipelines de procesamiento de datos, tanto en tiempo real como por lotes. **Google Cloud Dataflow** ejecuta estos pipelines en la nube, proporcionando escalabilidad y facilidad de uso. En este laboratorio, crear√°s un pipeline de conteo de palabras utilizando **Apache Beam** con Python y lo ejecutar√°s localmente y en **Dataflow**.\n",
        "\n",
        "üí° **Beneficio empresarial**: **Dataflow** permite a las empresas procesar grandes vol√∫menes de datos, como registros financieros o transacciones, de manera eficiente y escalable, reduciendo costos operativos y automatizando tareas de an√°lisis.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataflow](https://cloud.google.com/dataflow/docs) y la [Documentaci√≥n de Apache Beam](https://beam.apache.org/documentation/).\n",
        "\n",
        "## üéØ Objetivos\n",
        "- ü™£ Crear un bucket en **Cloud Storage**.\n",
        "- üêç Instalar el **Apache Beam SDK** para Python.\n",
        "- ‚öôÔ∏è Ejecutar un pipeline de conteo de palabras localmente y en **Dataflow**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîπ Tarea 1: Crear un Bucket en Cloud Storage\n",
        "\n",
        "Un bucket en **Cloud Storage** es un espacio de almacenamiento en la nube para archivos temporales y resultados del pipeline.\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **Cloud Storage** > **Buckets**.\n",
        "2. Haz clic en **Create bucket**.\n",
        "3. Configura:\n",
        "   - **Name**: Asigna un nombre √∫nico (por ejemplo, `mi-nombre-bucket`). ‚ö†Ô∏è El nombre es p√∫blico, no incluyas datos sensibles.\n",
        "   - **Location type**: `Multi-region`.\n",
        "   - **Location**: `us (Estados Unidos)`.\n",
        "4. Haz clic en **Create**.\n",
        "5. Si aparece el mensaje \"Public access will be prevented\", haz clic en **Confirm**.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El bucket almacenar√° archivos temporales y resultados generados por el pipeline de **Dataflow**.\n",
        "\n",
        "üí° **Contexto empresarial**: Un bucket es como un archivo digital donde se guardan documentos contables durante el procesamiento de datos financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî∏ Tarea 2: Instalar Apache Beam SDK para Python\n",
        "\n",
        "En esta tarea, configurar√°s un entorno de Python con el **Apache Beam SDK** para ejecutar pipelines.\n",
        "\n",
        "### Paso 2.1: Ejecutar Python 3.9 con Docker\n",
        "\n",
        "1. En **Cloud Shell**, ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID python:3.9 /bin/bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "| **Parte** | **Significado** |\n",
        "|-----------|-----------------|\n",
        "| `docker run` | Ejecuta un contenedor (entorno aislado). |\n",
        "| `-it` | Habilita interacci√≥n con el contenedor. |\n",
        "| `-e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID` | Pasa el ID del proyecto como variable para autenticaci√≥n en Google Cloud. |\n",
        "| `python:3.9` | Usa la imagen base de Python 3.9. |\n",
        "| `/bin/bash` | Abre una terminal dentro del contenedor. |\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Este comando inicia un contenedor con Python 3.9, necesario para instalar y usar el **Apache Beam SDK**.\n",
        "\n",
        "üí° **Contexto empresarial**: Usar un contenedor es como configurar una estaci√≥n de trabajo contable con todas las herramientas necesarias instaladas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2.2: Instalar Apache Beam SDK\n",
        "\n",
        "1. Dentro del contenedor, ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install 'apache-beam[gcp]'==2.42.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "- `pip`: Gestor de paquetes de Python.\n",
        "- `'apache-beam[gcp]'`: Instala **Apache Beam** con soporte para Google Cloud.\n",
        "- `==2.42.0`: Especifica la versi√≥n exacta del SDK.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El SDK permite definir y ejecutar pipelines de datos con **Apache Beam**.\n",
        "- Ignora cualquier advertencia de dependencias.\n",
        "\n",
        "üí° **Contexto empresarial**: Instalar el SDK es como adquirir un software contable especializado para procesar transacciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2.3: Ejecutar ejemplo local de conteo de palabras\n",
        "\n",
        "1. Ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "python -m apache_beam.examples.wordcount --output OUTPUT_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explicaci√≥n**:\n",
        "- Este comando ejecuta un pipeline de ejemplo que cuenta palabras en un archivo de texto.\n",
        "- Usa el **DirectRunner** (ejecuci√≥n local) por defecto.\n",
        "- Ignora el mensaje: `INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.`\n",
        "\n",
        "2. Verifica el archivo de salida:\n",
        "   - Lista los archivos generados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "   - Visualiza el contenido del archivo de salida (reemplaza `<nombre del archivo>` con el nombre real):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cat <nombre del archivo>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultado esperado**:\n",
        "- Ver√°s un conteo de palabras, por ejemplo:\n",
        "  ```\n",
        "  word1: 2\n",
        "  word2: 4\n",
        "  ```\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El pipeline cuenta cu√°ntas veces aparece cada palabra en un texto.\n",
        "\n",
        "üí° **Contexto empresarial**: Este pipeline es como un informe que cuenta la frecuencia de transacciones por categor√≠a, ejecutado localmente para pruebas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî∏ Tarea 3: Ejecutar un pipeline en Dataflow (en la nube)\n",
        "\n",
        "En esta tarea, ejecutar√°s el mismo pipeline de conteo de palabras, pero en **Dataflow** para procesamiento en la nube.\n",
        "\n",
        "1. Define la variable de entorno para el bucket:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BUCKET=gs://<nombre-del-bucket>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explicaci√≥n**:\n",
        "- `BUCKET` almacena la ruta completa del bucket (por ejemplo, `gs://mi-nombre-bucket`).\n",
        "- El prefijo `gs://` indica que es una ubicaci√≥n en **Cloud Storage**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Ejecuta el pipeline en **Dataflow**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "python -m apache_beam.examples.wordcount --project $DEVSHELL_PROJECT_ID \\\n",
        "  --runner DataflowRunner \\\n",
        "  --staging_location $BUCKET/staging \\\n",
        "  --temp_location $BUCKET/temp \\\n",
        "  --output $BUCKET/results/output \\\n",
        "  --region \"us-central1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "| **Parte** | **Significado** |\n",
        "|-----------|-----------------|\n",
        "| `python -m apache_beam.examples.wordcount` | Ejecuta el pipeline de conteo de palabras. |\n",
        "| `--project $DEVSHELL_PROJECT_ID` | Especifica el proyecto de Google Cloud. |\n",
        "| `--runner DataflowRunner` | Usa **Dataflow** como motor de ejecuci√≥n en la nube. |\n",
        "| `--staging_location $BUCKET/staging` | Carpeta para archivos temporales de preparaci√≥n. |\n",
        "| `--temp_location $BUCKET/temp` | Carpeta para archivos intermedios. |\n",
        "| `--output $BUCKET/results/output` | Carpeta donde se guardan los resultados. |\n",
        "| `--region \"us-central1\"` | Regi√≥n donde se ejecuta el pipeline. |\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Este comando ejecuta el pipeline en la nube, utilizando los recursos escalables de **Dataflow**.\n",
        "- Espera hasta que aparezca el mensaje: `JOB_MESSAGE_DETAILED: Workers have started successfully.`\n",
        "\n",
        "üí° **Contexto empresarial**: Ejecutar en **Dataflow** es como procesar un gran volumen de transacciones financieras en un sistema centralizado en la nube."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî∏ Tarea 4: Verificar si el pipeline finaliz√≥ con √©xito\n",
        "\n",
        "1. Ve a **Navigation menu** ‚ò∞ > **Dataflow**.\n",
        "2. Busca el trabajo llamado `wordcount`.\n",
        "3. Verifica el estado:\n",
        "   - Inicialmente estar√° como **Running**.\n",
        "   - Haz clic para ver detalles.\n",
        "   - Cuando aparezca como **Succeeded**, el pipeline ha finalizado correctamente.\n",
        "4. Verifica los resultados en **Cloud Storage**:\n",
        "   - Ve a tu bucket.\n",
        "   - Confirma que existan las carpetas `staging`, `temp`, y `results`.\n",
        "   - En la carpeta `results`, abre un archivo para ver los conteos de palabras.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Los resultados en **Cloud Storage** confirman que el pipeline proces√≥ los datos correctamente.\n",
        "\n",
        "üí° **Contexto empresarial**: Verificar el pipeline es como revisar un informe contable para confirmar que todas las transacciones se procesaron correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Tarea 5: Pregunta de repaso\n",
        "\n",
        "| **Pregunta** | **Respuesta** |\n",
        "|--------------|---------------|\n",
        "| ¬øLa ubicaci√≥n temporal (`temp_location`) de Dataflow debe ser una URL v√°lida de Cloud Storage? | ‚úÖ Verdadero (True) |\n",
        "\n",
        "## üöÄ Resumen\n",
        "\n",
        "| **Concepto** | **Explicaci√≥n contable simplificada** |\n",
        "|--------------|--------------------------------------|\n",
        "| **Apache Beam** | Framework para definir procesos autom√°ticos de an√°lisis financiero. |\n",
        "| **Dataflow** | Sistema en la nube que ejecuta procesos financieros a gran escala. |\n",
        "| **Cloud Storage** | Archivo digital para guardar documentos financieros durante el procesamiento. |\n",
        "| **Pipeline** | Proceso automatizado que analiza datos financieros, como conteos de transacciones. |\n",
        "\n",
        "üí° **Conclusi√≥n empresarial**: **Dataflow** con **Apache Beam** permite a las empresas automatizar el procesamiento de datos financieros, como conteos o res√∫menes de transacciones, con escalabilidad y eficiencia. Esto reduce costos operativos y facilita an√°lisis r√°pidos para la toma de decisiones.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataflow](https://cloud.google.com/dataflow/docs) y la [Documentaci√≥n de Apache Beam](https://beam.apache.org/documentation/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}