{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📌 Laboratorio: Dataproc – Inicio Rápido desde la Consola\n",
        "\n",
        "## 🌐 Introducción General\n",
        "**Dataproc** es un servicio gestionado de **Google Cloud** que permite ejecutar clústeres de **Apache Spark** y **Apache Hadoop** de forma rápida, sencilla y rentable. Operaciones que antes tomaban horas o días ahora se completan en segundos o minutos.\n",
        "\n",
        "💡 **Contexto empresarial**: Similar a contratar una oficina contable externa para procesar grandes volúmenes de transacciones financieras de forma eficiente, escalable y sin costos fijos innecesarios.\n",
        "\n",
        "### Objetivos del Laboratorio\n",
        "- Crear un clúster de **Dataproc** desde la consola de **Google Cloud**.\n",
        "- Ejecutar un **Spark job** básico para calcular el valor de π (pi).\n",
        "- Modificar la cantidad de **worker nodes** en el clúster.\n",
        "\n",
        "Para más información, consulta la [Documentación de Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Configuración y Requisitos\n",
        "\n",
        "### Antes de Iniciar\n",
        "- Este laboratorio está cronometrado y no se puede pausar.\n",
        "- Usa una cuenta temporal de **Google Cloud** con credenciales proporcionadas.\n",
        "- Utiliza una ventana de incógnito (recomendado) para evitar conflictos con cuentas personales.\n",
        "- **No uses cuentas personales** para evitar cargos inesperados.\n",
        "\n",
        "### Inicio de Sesión\n",
        "1. Haz clic en **Start Lab** en la plataforma del laboratorio.\n",
        "2. Copia el **Username** y **Password** proporcionados.\n",
        "3. Inicia sesión en la consola de **Google Cloud**.\n",
        "4. Acepta los términos y condiciones sin configurar recuperación ni autenticación en dos pasos.\n",
        "\n",
        "💡 **Propósito**: Garantizar un entorno seguro y controlado, como preparar un sistema contable temporal para una auditoría."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🛠️ Tarea 1: Confirmar API y Permisos\n",
        "\n",
        "### Habilitar la API de Dataproc\n",
        "1. Ve a **Navigation menu > APIs & Services > Library**.\n",
        "2. Busca **Cloud Dataproc**.\n",
        "3. Si no está habilitada, haz clic en **Enable**.\n",
        "\n",
        "### Asignar Permisos de Almacenamiento\n",
        "1. Ve a **Navigation menu > IAM & Admin > IAM**.\n",
        "2. Edita la cuenta `compute@developer.gserviceaccount.com`.\n",
        "3. Agrega el rol **Storage Admin**.\n",
        "4. Guarda los cambios.\n",
        "\n",
        "💡 **Contexto empresarial**: Habilitar la API y asignar permisos es como autorizar a un equipo contable para acceder a los registros financieros almacenados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Tarea 2: Crear un Clúster\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Navigation menu > View all products > Dataproc > Clusters**.\n",
        "2. Haz clic en **Create Cluster > Cluster on Compute Engine**.\n",
        "3. Configura los siguientes campos:\n",
        "   - **Name**: `example-cluster`\n",
        "   - **Region**: Selecciona la región asignada\n",
        "   - **Zone**: Selecciona la zona asignada\n",
        "   - **Primary disk type (Manager)**: Standard Persistent Disk\n",
        "   - **Machine Series (Manager)**: E2\n",
        "   - **Machine Type (Manager)**: `e2-standard-2`\n",
        "   - **Primary disk size (Manager)**: 30 GB\n",
        "   - **Number of Worker Nodes**: 2\n",
        "   - **Primary disk type (Workers)**: Standard Persistent Disk\n",
        "   - **Machine Series (Workers)**: E2\n",
        "   - **Machine Type (Workers)**: `e2-standard-2`\n",
        "   - **Primary disk size (Workers)**: 30 GB\n",
        "   - **Internal IP only**: Desmarcar\n",
        "4. Haz clic en **Create**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para crear el clúster desde **Cloud Shell** (reemplaza `REGION` y `ZONE` con valores específicos, ej. `us-central1`, `us-central1-a`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc clusters create example-cluster \\\n    --region=REGION \\\n    --zone=ZONE \\\n    --master-machine-type=e2-standard-2 \\\n    --master-disk-type=pd-standard \\\n    --master-disk-size=30GB \\\n    --num-workers=2 \\\n    --worker-machine-type=e2-standard-2 \\\n    --worker-disk-type=pd-standard \\\n    --worker-disk-size=30GB \\\n    --no-address",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: Crear un clúster es como montar una oficina contable temporal para procesar grandes volúmenes de datos financieros, escalable según necesidades.\n",
        "\n",
        "💡 **Beneficio empresarial**: Optimiza costos al usar recursos solo cuando se necesitan, como contratar personal temporal para una auditoría masiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🛠️ Tarea 3: Ejecutar un Spark Job\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Jobs** en el menú izquierdo de **Dataproc**.\n",
        "2. Haz clic en **Submit Job**.\n",
        "3. Configura los siguientes campos:\n",
        "   - **Region**: Selecciona la región asignada\n",
        "   - **Cluster**: `example-cluster`\n",
        "   - **Job type**: Spark\n",
        "   - **Main class or jar**: `org.apache.spark.examples.SparkPi`\n",
        "   - **Jar files**: `file:///usr/lib/spark/examples/jars/spark-examples.jar`\n",
        "   - **Arguments**: `1000`\n",
        "4. Haz clic en **Submit**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para enviar el job desde **Cloud Shell**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc jobs submit spark \\\n    --region=REGION \\\n    --cluster=example-cluster \\\n    --class=org.apache.spark.examples.SparkPi \\\n    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n    -- 1000",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: Este job usa el método de Monte Carlo para estimar π, distribuyendo el cálculo entre los **worker nodes**. Es como dividir un cálculo contable complejo entre varios contadores para mayor rapidez.\n",
        "\n",
        "💡 **Beneficio empresarial**: Procesa grandes volúmenes de datos (ej. transacciones financieras) en paralelo, reduciendo el tiempo de análisis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Tarea 4: Ver el Resultado del Job\n",
        "\n",
        "### Pasos\n",
        "1. Haz clic en el **Job ID** en el listado de jobs.\n",
        "2. Activa la opción **LINE WRAP ON**.\n",
        "3. Busca la estimación de π en el resultado.\n",
        "\n",
        "💡 **Contexto empresarial**: Verificar el resultado es como revisar un informe financiero generado automáticamente para confirmar su precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Tarea 5: Modificar el Número de Workers\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Clusters** y selecciona `example-cluster`.\n",
        "2. Ve a la pestaña **Configuration > Edit**.\n",
        "3. Cambia **Worker Nodes** a 4.\n",
        "4. Haz clic en **Save**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para actualizar el número de workers desde **Cloud Shell**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc clusters update example-cluster \\\n    --region=REGION \\\n    --num-workers=4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "💡 **Propósito**: Aumentar los workers es como contratar más contadores para acelerar un proceso contable intensivo.\n",
        "\n",
        "💡 **Beneficio empresarial**: Escala recursos según la carga de trabajo, optimizando costos y rendimiento.\n",
        "\n",
        "### Rerun del Job (Opcional)\n",
        "1. Ve a **Jobs > Submit Job**.\n",
        "2. Repite los pasos de la Tarea 3 para enviar el mismo job.\n",
        "\n",
        "💡 **Beneficio**: Comparar el rendimiento con más workers es como evaluar la eficiencia de un equipo contable ampliado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Tarea 6: Preguntas de Repaso\n",
        "\n",
        "1. **¿Qué tipo de Dataproc job se ejecutó en el laboratorio?**\n",
        "   - ✅ **Spark**\n",
        "\n",
        "2. **Dataproc ayuda a procesar, transformar y comprender grandes cantidades de datos.**\n",
        "   - ✅ **Verdadero**\n",
        "\n",
        "💡 **Contexto empresarial**: Estas preguntas refuerzan la comprensión de **Dataproc** como una herramienta para procesar datos financieros masivos de forma eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Analogía Contable\n",
        "**Dataproc** es como una oficina contable externa y temporal que contratas para cálculos masivos (ej. liquidación de impuestos con grandes volúmenes de datos). Beneficios:\n",
        "- **Ahorro de costos**: Solo pagas por el tiempo de uso.\n",
        "- **Escalabilidad**: Añades más trabajadores según necesidad.\n",
        "- **Herramientas estándar**: Usa **Spark** y **Hadoop**, como software contable conocido.\n",
        "\n",
        "💡 **Ejemplo**: Procesar transacciones de un año completo en minutos, en lugar de días, para una auditoría."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Cuadro Resumen Final\n",
        "\n",
        "| Elemento Clave | Descripción |\n",
        "|----------------|-------------|\n",
        "| **Servicio** | Google Cloud Dataproc |\n",
        "| **Objetivo** | Crear clúster, ejecutar Spark job, modificar workers |\n",
        "| **Herramienta** | Apache Spark sobre Dataproc |\n",
        "| **Tipo de Job** | Cálculo de π con Monte Carlo |\n",
        "| **Ventaja** | Rapidez, bajo costo, escalabilidad |\n",
        "| **Perspectiva contable** | Oficina temporal para cálculos complejos |\n",
        "| **Rol de Workers** | Ejecutan cálculos en paralelo |\n",
        "\n",
        "💡 **Conclusión empresarial**: **Dataproc** permite procesar grandes volúmenes de datos financieros de forma rápida y escalable, optimizando costos y recursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Conclusión\n",
        "\n",
        "Este laboratorio te permitió:\n",
        "1. Crear un clúster en **Dataproc** desde la consola.\n",
        "2. Ejecutar un **Spark job** para calcular π.\n",
        "3. Escalar el clúster modificando el número de **worker nodes**.\n",
        "\n",
        "💡 **Beneficio empresarial**: Automatiza y acelera el procesamiento de datos financieros, como consolidar transacciones o generar reportes, con costos optimizados.\n",
        "\n",
        "Para más información, consulta la [Documentación de Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}