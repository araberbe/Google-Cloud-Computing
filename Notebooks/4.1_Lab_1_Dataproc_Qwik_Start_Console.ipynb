{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìå Laboratorio: Dataproc ‚Äì Inicio R√°pido desde la Consola\n",
        "\n",
        "## üåê Introducci√≥n General\n",
        "**Dataproc** es un servicio gestionado de **Google Cloud** que permite ejecutar cl√∫steres de **Apache Spark** y **Apache Hadoop** de forma r√°pida, sencilla y rentable. Operaciones que antes tomaban horas o d√≠as ahora se completan en segundos o minutos.\n",
        "\n",
        "üí° **Contexto empresarial**: Similar a contratar una oficina contable externa para procesar grandes vol√∫menes de transacciones financieras de forma eficiente, escalable y sin costos fijos innecesarios.\n",
        "\n",
        "### Objetivos del Laboratorio\n",
        "- Crear un cl√∫ster de **Dataproc** desde la consola de **Google Cloud**.\n",
        "- Ejecutar un **Spark job** b√°sico para calcular el valor de œÄ (pi).\n",
        "- Modificar la cantidad de **worker nodes** en el cl√∫ster.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuraci√≥n y Requisitos\n",
        "\n",
        "### Antes de Iniciar\n",
        "- Este laboratorio est√° cronometrado y no se puede pausar.\n",
        "- Usa una cuenta temporal de **Google Cloud** con credenciales proporcionadas.\n",
        "- Utiliza una ventana de inc√≥gnito (recomendado) para evitar conflictos con cuentas personales.\n",
        "- **No uses cuentas personales** para evitar cargos inesperados.\n",
        "\n",
        "### Inicio de Sesi√≥n\n",
        "1. Haz clic en **Start Lab** en la plataforma del laboratorio.\n",
        "2. Copia el **Username** y **Password** proporcionados.\n",
        "3. Inicia sesi√≥n en la consola de **Google Cloud**.\n",
        "4. Acepta los t√©rminos y condiciones sin configurar recuperaci√≥n ni autenticaci√≥n en dos pasos.\n",
        "\n",
        "üí° **Prop√≥sito**: Garantizar un entorno seguro y controlado, como preparar un sistema contable temporal para una auditor√≠a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Tarea 1: Confirmar API y Permisos\n",
        "\n",
        "### Habilitar la API de Dataproc\n",
        "1. Ve a **Navigation menu > APIs & Services > Library**.\n",
        "2. Busca **Cloud Dataproc**.\n",
        "3. Si no est√° habilitada, haz clic en **Enable**.\n",
        "\n",
        "### Asignar Permisos de Almacenamiento\n",
        "1. Ve a **Navigation menu > IAM & Admin > IAM**.\n",
        "2. Edita la cuenta `compute@developer.gserviceaccount.com`.\n",
        "3. Agrega el rol **Storage Admin**.\n",
        "4. Guarda los cambios.\n",
        "\n",
        "üí° **Contexto empresarial**: Habilitar la API y asignar permisos es como autorizar a un equipo contable para acceder a los registros financieros almacenados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Tarea 2: Crear un Cl√∫ster\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Navigation menu > View all products > Dataproc > Clusters**.\n",
        "2. Haz clic en **Create Cluster > Cluster on Compute Engine**.\n",
        "3. Configura los siguientes campos:\n",
        "   - **Name**: `example-cluster`\n",
        "   - **Region**: Selecciona la regi√≥n asignada\n",
        "   - **Zone**: Selecciona la zona asignada\n",
        "   - **Primary disk type (Manager)**: Standard Persistent Disk\n",
        "   - **Machine Series (Manager)**: E2\n",
        "   - **Machine Type (Manager)**: `e2-standard-2`\n",
        "   - **Primary disk size (Manager)**: 30 GB\n",
        "   - **Number of Worker Nodes**: 2\n",
        "   - **Primary disk type (Workers)**: Standard Persistent Disk\n",
        "   - **Machine Series (Workers)**: E2\n",
        "   - **Machine Type (Workers)**: `e2-standard-2`\n",
        "   - **Primary disk size (Workers)**: 30 GB\n",
        "   - **Internal IP only**: Desmarcar\n",
        "4. Haz clic en **Create**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para crear el cl√∫ster desde **Cloud Shell** (reemplaza `REGION` y `ZONE` con valores espec√≠ficos, ej. `us-central1`, `us-central1-a`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc clusters create example-cluster \\\n    --region=REGION \\\n    --zone=ZONE \\\n    --master-machine-type=e2-standard-2 \\\n    --master-disk-type=pd-standard \\\n    --master-disk-size=30GB \\\n    --num-workers=2 \\\n    --worker-machine-type=e2-standard-2 \\\n    --worker-disk-type=pd-standard \\\n    --worker-disk-size=30GB \\\n    --no-address",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Crear un cl√∫ster es como montar una oficina contable temporal para procesar grandes vol√∫menes de datos financieros, escalable seg√∫n necesidades.\n",
        "\n",
        "üí° **Beneficio empresarial**: Optimiza costos al usar recursos solo cuando se necesitan, como contratar personal temporal para una auditor√≠a masiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Tarea 3: Ejecutar un Spark Job\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Jobs** en el men√∫ izquierdo de **Dataproc**.\n",
        "2. Haz clic en **Submit Job**.\n",
        "3. Configura los siguientes campos:\n",
        "   - **Region**: Selecciona la regi√≥n asignada\n",
        "   - **Cluster**: `example-cluster`\n",
        "   - **Job type**: Spark\n",
        "   - **Main class or jar**: `org.apache.spark.examples.SparkPi`\n",
        "   - **Jar files**: `file:///usr/lib/spark/examples/jars/spark-examples.jar`\n",
        "   - **Arguments**: `1000`\n",
        "4. Haz clic en **Submit**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para enviar el job desde **Cloud Shell**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc jobs submit spark \\\n    --region=REGION \\\n    --cluster=example-cluster \\\n    --class=org.apache.spark.examples.SparkPi \\\n    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n    -- 1000",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Este job usa el m√©todo de Monte Carlo para estimar œÄ, distribuyendo el c√°lculo entre los **worker nodes**. Es como dividir un c√°lculo contable complejo entre varios contadores para mayor rapidez.\n",
        "\n",
        "üí° **Beneficio empresarial**: Procesa grandes vol√∫menes de datos (ej. transacciones financieras) en paralelo, reduciendo el tiempo de an√°lisis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Tarea 4: Ver el Resultado del Job\n",
        "\n",
        "### Pasos\n",
        "1. Haz clic en el **Job ID** en el listado de jobs.\n",
        "2. Activa la opci√≥n **LINE WRAP ON**.\n",
        "3. Busca la estimaci√≥n de œÄ en el resultado.\n",
        "\n",
        "üí° **Contexto empresarial**: Verificar el resultado es como revisar un informe financiero generado autom√°ticamente para confirmar su precisi√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Tarea 5: Modificar el N√∫mero de Workers\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Clusters** y selecciona `example-cluster`.\n",
        "2. Ve a la pesta√±a **Configuration > Edit**.\n",
        "3. Cambia **Worker Nodes** a 4.\n",
        "4. Haz clic en **Save**.\n",
        "\n",
        "### Comando Equivalente\n",
        "Para actualizar el n√∫mero de workers desde **Cloud Shell**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud dataproc clusters update example-cluster \\\n    --region=REGION \\\n    --num-workers=4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Prop√≥sito**: Aumentar los workers es como contratar m√°s contadores para acelerar un proceso contable intensivo.\n",
        "\n",
        "üí° **Beneficio empresarial**: Escala recursos seg√∫n la carga de trabajo, optimizando costos y rendimiento.\n",
        "\n",
        "### Rerun del Job (Opcional)\n",
        "1. Ve a **Jobs > Submit Job**.\n",
        "2. Repite los pasos de la Tarea 3 para enviar el mismo job.\n",
        "\n",
        "üí° **Beneficio**: Comparar el rendimiento con m√°s workers es como evaluar la eficiencia de un equipo contable ampliado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Tarea 6: Preguntas de Repaso\n",
        "\n",
        "1. **¬øQu√© tipo de Dataproc job se ejecut√≥ en el laboratorio?**\n",
        "   - ‚úÖ **Spark**\n",
        "\n",
        "2. **Dataproc ayuda a procesar, transformar y comprender grandes cantidades de datos.**\n",
        "   - ‚úÖ **Verdadero**\n",
        "\n",
        "üí° **Contexto empresarial**: Estas preguntas refuerzan la comprensi√≥n de **Dataproc** como una herramienta para procesar datos financieros masivos de forma eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Analog√≠a Contable\n",
        "**Dataproc** es como una oficina contable externa y temporal que contratas para c√°lculos masivos (ej. liquidaci√≥n de impuestos con grandes vol√∫menes de datos). Beneficios:\n",
        "- **Ahorro de costos**: Solo pagas por el tiempo de uso.\n",
        "- **Escalabilidad**: A√±ades m√°s trabajadores seg√∫n necesidad.\n",
        "- **Herramientas est√°ndar**: Usa **Spark** y **Hadoop**, como software contable conocido.\n",
        "\n",
        "üí° **Ejemplo**: Procesar transacciones de un a√±o completo en minutos, en lugar de d√≠as, para una auditor√≠a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cuadro Resumen Final\n",
        "\n",
        "| Elemento Clave | Descripci√≥n |\n",
        "|----------------|-------------|\n",
        "| **Servicio** | Google Cloud Dataproc |\n",
        "| **Objetivo** | Crear cl√∫ster, ejecutar Spark job, modificar workers |\n",
        "| **Herramienta** | Apache Spark sobre Dataproc |\n",
        "| **Tipo de Job** | C√°lculo de œÄ con Monte Carlo |\n",
        "| **Ventaja** | Rapidez, bajo costo, escalabilidad |\n",
        "| **Perspectiva contable** | Oficina temporal para c√°lculos complejos |\n",
        "| **Rol de Workers** | Ejecutan c√°lculos en paralelo |\n",
        "\n",
        "üí° **Conclusi√≥n empresarial**: **Dataproc** permite procesar grandes vol√∫menes de datos financieros de forma r√°pida y escalable, optimizando costos y recursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Conclusi√≥n\n",
        "\n",
        "Este laboratorio te permiti√≥:\n",
        "1. Crear un cl√∫ster en **Dataproc** desde la consola.\n",
        "2. Ejecutar un **Spark job** para calcular œÄ.\n",
        "3. Escalar el cl√∫ster modificando el n√∫mero de **worker nodes**.\n",
        "\n",
        "üí° **Beneficio empresarial**: Automatiza y acelera el procesamiento de datos financieros, como consolidar transacciones o generar reportes, con costos optimizados.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Dataproc](https://cloud.google.com/dataproc/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}