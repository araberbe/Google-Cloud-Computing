{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Œ Laboratorio: Dataflow Qwik Start - Python\n",
        "\n",
        "## ðŸŒ DescripciÃ³n General\n",
        "**Apache Beam SDK** es un modelo de programaciÃ³n de cÃ³digo abierto para crear canalizaciones (pipelines) de datos. En **Google Cloud**, puedes definir pipelines con **Apache Beam** y ejecutarlos en **Dataflow**. Este laboratorio te guÃ­a para configurar un entorno de desarrollo en Python, instalar **Apache Beam**, y ejecutar un pipeline remotamente.\n",
        "\n",
        "ðŸ’¡ **Contexto empresarial**: Similar a configurar un sistema automatizado que procesa transacciones financieras, como facturas o registros de ventas, y genera reportes consolidados en tiempo rÃ©cord.\n",
        "\n",
        "### Objetivos del Laboratorio\n",
        "- Crear un **bucket** en **Cloud Storage** para almacenar resultados.\n",
        "- Instalar el **Apache Beam SDK** para Python.\n",
        "- Ejecutar un pipeline de **Dataflow** remotamente.\n",
        "\n",
        "Para mÃ¡s informaciÃ³n, consulta la [DocumentaciÃ³n de Dataflow](https://cloud.google.com/dataflow/docs) y [DocumentaciÃ³n de Apache Beam](https://beam.apache.org/documentation/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Requisitos Previos y ConfiguraciÃ³n Inicial\n",
        "\n",
        "### Antes de Comenzar\n",
        "- Usa un entorno real de **Google Cloud** con credenciales temporales.\n",
        "- No pauses el laboratorio una vez iniciado.\n",
        "- Usa la cuenta estudiantil proporcionada, no una cuenta personal.\n",
        "- Abre el navegador en modo incÃ³gnito para evitar conflictos.\n",
        "\n",
        "### Iniciar el Laboratorio\n",
        "1. Haz clic en **Start Lab** en la plataforma del laboratorio.\n",
        "2. Copia las credenciales temporales (**usuario** y **contraseÃ±a**).\n",
        "3. Haz clic en **Open Google Cloud Console**.\n",
        "4. Inicia sesiÃ³n, acepta los tÃ©rminos y condiciones, y no configures recuperaciÃ³n ni autenticaciÃ³n en dos pasos.\n",
        "\n",
        "### Activar Cloud Shell\n",
        "**Cloud Shell** es una mÃ¡quina virtual con herramientas preinstaladas para desarrolladores.\n",
        "\n",
        "#### Pasos\n",
        "1. Haz clic en **Activate Cloud Shell** en la consola.\n",
        "2. Acepta los permisos.\n",
        "\n",
        "#### Comandos Opcionales\n",
        "Verifica la cuenta y el proyecto activos, y configura la regiÃ³n (reemplaza `REGION` con un valor especÃ­fico, ej. `us-central1`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gcloud auth list\ngcloud config list project\ngcloud config set compute/region REGION",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: Confirma el proyecto y la regiÃ³n, como verificar el libro contable y la sucursal antes de procesar transacciones.\n",
        "\n",
        "### Verificar la API de Dataflow\n",
        "1. Busca **Dataflow API** en la barra de bÃºsqueda de la consola.\n",
        "2. Haz clic en **Manage**.\n",
        "3. Deshabilita y vuelve a habilitar la API.\n",
        "\n",
        "ðŸ’¡ **PropÃ³sito**: Asegura que el servicio estÃ© activo, como validar un sistema contable antes de usarlo.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Garantiza que el pipeline funcione sin errores, optimizando procesos financieros.\n",
        "\n",
        "Para mÃ¡s informaciÃ³n, consulta la [DocumentaciÃ³n de Cloud Shell](https://cloud.google.com/shell/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Tarea 1: Crear un Bucket de Cloud Storage\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Cloud Storage > Buckets** en el menÃº de navegaciÃ³n.\n",
        "2. Haz clic en **Create bucket**.\n",
        "3. Configura:\n",
        "   - **Name**: Un nombre Ãºnico (ej. `nombre-bucket`).\n",
        "   - **Location type**: Multi-region.\n",
        "   - **Location**: `us`.\n",
        "4. Confirma la prevenciÃ³n de acceso pÃºblico.\n",
        "\n",
        "### Comando Equivalente\n",
        "Crea el bucket desde **Cloud Shell** (reemplaza `nombre-bucket` con un nombre Ãºnico):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "gsutil mb gs://nombre-bucket/",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: El bucket almacena resultados del pipeline, como una carpeta temporal para documentos financieros antes de procesarlos.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Proporciona almacenamiento escalable y seguro para datos intermedios.\n",
        "\n",
        "Para mÃ¡s informaciÃ³n, consulta la [DocumentaciÃ³n de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ› ï¸ Tarea 2: Instalar Apache Beam SDK para Python\n",
        "\n",
        "### Paso 1: Ejecutar Python 3.9 desde Docker\n",
        "Inicia un contenedor con Python 3.9:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID python:3.9 /bin/bash",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: Crea un entorno aislado con Python 3.9, como configurar un software contable especÃ­fico para un proyecto.\n",
        "\n",
        "### Paso 2: Instalar Apache Beam\n",
        "Instala el SDK con soporte para Google Cloud:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "pip install 'apache-beam[gcp]'==2.42.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: Prepara el entorno para desarrollar pipelines, como instalar un mÃ³dulo de anÃ¡lisis financiero.\n",
        "\n",
        "### Paso 3: Ejecutar Ejemplo Localmente\n",
        "Prueba el ejemplo `wordcount` (reemplaza `OUTPUT_FILE` con un nombre, ej. `output.txt`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "python -m apache_beam.examples.wordcount --output OUTPUT_FILE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifica los resultados:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ls\ncat OUTPUT_FILE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: Ejecuta un pipeline localmente para contar palabras, como probar un sistema contable con datos de muestra.\n",
        "\n",
        "ðŸ’¡ **AnalogÃ­a contable**: Similar a contar transacciones por categorÃ­a (ej. ingresos por producto) antes de procesarlas a gran escala.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Valida el entorno antes de ejecutar pipelines complejos, asegurando precisiÃ³n en anÃ¡lisis financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ Tarea 3: Ejecutar la CanalizaciÃ³n de Dataflow Remotamente\n",
        "\n",
        "### Paso 1: Definir Variable BUCKET\n",
        "Configura la variable del bucket (reemplaza `nombre-del-bucket` con el nombre creado):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "BUCKET=gs://nombre-del-bucket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **PropÃ³sito**: Define la ubicaciÃ³n del bucket, como especificar la carpeta donde se guardarÃ¡n los reportes financieros.\n",
        "\n",
        "### Paso 2: Ejecutar el Pipeline\n",
        "Ejecuta el pipeline `wordcount` en **Dataflow** (reemplaza `REGION` con un valor, ej. `us-central1`):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "python -m apache_beam.examples.wordcount --project $DEVSHELL_PROJECT_ID \\\n  --runner DataflowRunner \\\n  --staging_location $BUCKET/staging \\\n  --temp_location $BUCKET/temp \\\n  --output $BUCKET/results/output \\\n  --region REGION",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| ParÃ¡metro | FunciÃ³n |\n",
        "|-----------|---------|\n",
        "| `--project` | ID del proyecto activo. |\n",
        "| `--runner DataflowRunner` | Ejecuta el pipeline en **Dataflow**. |\n",
        "| `--staging_location` | Carpeta temporal para archivos del entorno. |\n",
        "| `--temp_location` | Almacena archivos temporales del proceso. |\n",
        "| `--output` | Directorio para resultados. |\n",
        "| `--region` | RegiÃ³n de ejecuciÃ³n. |\n",
        "\n",
        "ðŸ’¡ **PropÃ³sito**: Ejecuta el pipeline remotamente, procesando datos a gran escala, como consolidar transacciones de mÃºltiples sucursales.\n",
        "\n",
        "ðŸ’¡ **AnalogÃ­a contable**: Como configurar un sistema automÃ¡tico para procesar y resumir miles de registros financieros en la nube.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Automatiza el anÃ¡lisis de datos, reduciendo tiempo y costos en reportes financieros.\n",
        "\n",
        "Espera el mensaje: **â€œJOB_MESSAGE_DETAILED: Workers have started successfully.â€**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Tarea 4: Verificar el Ã‰xito del Job\n",
        "\n",
        "### Pasos\n",
        "1. Ve a **Dataflow** en el menÃº de navegaciÃ³n.\n",
        "2. Busca el job `wordcount`.\n",
        "3. Verifica que el estado sea **Succeeded**.\n",
        "4. Ve a **Cloud Storage > Buckets**.\n",
        "5. Abre el bucket y revisa las carpetas `results` y `staging`.\n",
        "6. Abre un archivo en `results` para ver el conteo de palabras.\n",
        "\n",
        "ðŸ’¡ **PropÃ³sito**: Confirma que el pipeline procesÃ³ los datos correctamente, como revisar un informe financiero para validar su precisiÃ³n.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Garantiza resultados confiables para anÃ¡lisis financieros o auditorÃ­as."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Tarea 5: Pregunta de VerificaciÃ³n\n",
        "\n",
        "**Pregunta**: La ubicaciÃ³n temporal de **Dataflow** (`temp_location`) debe ser una URL vÃ¡lida de **Cloud Storage**.\n",
        "- âœ… **Verdadero**\n",
        "\n",
        "ðŸ’¡ **Contexto empresarial**: Esto asegura que los datos intermedios se almacenen correctamente, como guardar documentos temporales en un archivador seguro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‹ Cuadro Resumen Final\n",
        "\n",
        "| Elemento | DescripciÃ³n |\n",
        "|----------|-------------|\n",
        "| **Producto principal** | Dataflow + Apache Beam |\n",
        "| **Lenguaje usado** | Python |\n",
        "| **Objetivo** | Crear y ejecutar un pipeline de procesamiento de datos |\n",
        "| **Almacenamiento** | Google Cloud Storage (bucket) |\n",
        "| **Entorno** | Cloud Shell + contenedor Docker con Python 3.9 |\n",
        "| **Herramientas** | gcloud, docker, pip, Apache Beam SDK |\n",
        "| **EjecuciÃ³n** | Local (DirectRunner) y Remota (DataflowRunner) |\n",
        "| **Resultado** | Archivo con conteo de palabras |\n",
        "\n",
        "ðŸ’¡ **ConclusiÃ³n empresarial**: Este laboratorio automatiza el procesamiento de datos, como consolidar transacciones financieras, optimizando eficiencia y costos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ ConclusiÃ³n\n",
        "\n",
        "Este laboratorio te permitiÃ³:\n",
        "1. Crear un **bucket** en **Cloud Storage**.\n",
        "2. Instalar **Apache Beam SDK** en un entorno Docker.\n",
        "3. Ejecutar un pipeline de **Dataflow** remotamente.\n",
        "4. Verificar los resultados en **Cloud Storage**.\n",
        "\n",
        "ðŸ’¡ **Beneficio empresarial**: Automatiza procesos ETL para datos financieros, como procesar registros de ventas en tiempo real, reduciendo errores y acelerando reportes.\n",
        "\n",
        "Para mÃ¡s informaciÃ³n, consulta la [DocumentaciÃ³n de Dataflow](https://cloud.google.com/dataflow/docs), [DocumentaciÃ³n de Apache Beam](https://beam.apache.org/documentation/), y [DocumentaciÃ³n de Cloud Storage](https://cloud.google.com/storage/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Bash",
      "language": "bash",
      "name": "bash"
    },
    "language_info": {
      "name": "bash"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}