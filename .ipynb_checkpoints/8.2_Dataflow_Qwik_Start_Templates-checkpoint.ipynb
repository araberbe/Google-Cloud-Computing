{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Laboratorio: Inicio R√°pido con Plantillas de Dataflow\n",
        "\n",
        "**Google Cloud Dataflow** es un servicio serverless para procesar datos en tiempo real o por lotes, utilizando pipelines escalables. En este laboratorio, crear√°s un pipeline de streaming utilizando la plantilla **Pub/Sub to BigQuery**, que lee mensajes JSON desde un tema de **Pub/Sub** y los inserta en una tabla de **BigQuery**. Este enfoque es ideal para procesar datos en tiempo real, como transacciones financieras o registros de auditor√≠a.\n",
        "\n",
        "üí° **Beneficio empresarial**: **Dataflow** permite a las empresas procesar grandes vol√∫menes de datos financieros en tiempo real, como registros de transacciones, asegurando an√°lisis r√°pidos, escalabilidad y cumplimiento normativo con m√≠nima configuraci√≥n manual.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataflow](https://cloud.google.com/dataflow/docs) y la [Documentaci√≥n de Google Cloud BigQuery](https://cloud.google.com/bigquery/docs).\n",
        "\n",
        "## üéØ Objetivo principal\n",
        "- Crear un pipeline de streaming utilizando la plantilla **Pub/Sub to BigQuery**, que:\n",
        "  1. Lee mensajes JSON desde un tema de **Pub/Sub** (mensajer√≠a en tiempo real).\n",
        "  2. Inserta los mensajes en una tabla de **BigQuery** (base de datos anal√≠tica).\n",
        "\n",
        "## üß≠ Actividades\n",
        "- ‚úîÔ∏è Crear un dataset y tabla en **BigQuery**.\n",
        "- ‚úîÔ∏è Crear un bucket en **Cloud Storage**.\n",
        "- ‚úîÔ∏è Configurar y ejecutar un pipeline de streaming con **Dataflow**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Tarea 1: Reiniciar la API de Dataflow\n",
        "\n",
        "Para asegurar que la **API de Dataflow** est√© habilitada y operativa:\n",
        "\n",
        "1. En la barra superior de b√∫squeda de **Google Cloud Console**, escribe `Dataflow API`.\n",
        "2. Haz clic en el resultado y selecciona **Manage**.\n",
        "3. Haz clic en **Disable API** y confirma.\n",
        "4. Luego, haz clic en **Enable API** para reactivarla.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Reiniciar la API asegura que **Dataflow** est√© correctamente habilitado en el proyecto.\n",
        "\n",
        "üí° **Contexto empresarial**: Habilitar la API es como activar un m√≥dulo de software contable para procesar transacciones autom√°ticamente.\n",
        "\n",
        "‚úÖ **Verificaci√≥n**: Haz clic en **Check my progress** para validar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÇÔ∏è Tarea 2: Crear dataset, tabla en BigQuery y bucket con Cloud Shell\n",
        "\n",
        "En esta tarea, preparar√°s los recursos necesarios usando comandos en **Cloud Shell**.\n",
        "\n",
        "### Paso 2.1: Crear dataset en BigQuery\n",
        "\n",
        "1. En **Cloud Shell**, ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bq mk taxirides"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "- `bq`: Herramienta de l√≠nea de comandos para **BigQuery**.\n",
        "- `mk`: Comando para crear (\"make\").\n",
        "- `taxirides`: Nombre del dataset.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El dataset `taxirides` ser√° el contenedor para la tabla que almacenar√° los datos del pipeline.\n",
        "\n",
        "üí° **Contexto empresarial**: Un dataset es como una carpeta en un sistema contable donde se organizan los registros financieros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2.2: Crear tabla en BigQuery\n",
        "\n",
        "1. Ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bq mk \\\n",
        "--time_partitioning_field timestamp \\\n",
        "--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer -t taxirides.realtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "| **Parte** | **Significado** |\n",
        "|-----------|-----------------|\n",
        "| `bq mk` | Crear una tabla en **BigQuery**. |\n",
        "| `--time_partitioning_field timestamp` | Particiona los datos por el campo `timestamp` para optimizar consultas. |\n",
        "| `--schema ...` | Define el esquema con columnas y tipos de datos: `ride_id:string`, `point_idx:integer`, `latitude:float`, `longitude:float`, `timestamp:timestamp`, `meter_reading:float`, `meter_increment:float`, `ride_status:string`, `passenger_count:integer`. |\n",
        "| `-t taxirides.realtime` | Crea la tabla `realtime` dentro del dataset `taxirides`. |\n",
        "\n",
        "**Ejemplo de campo**:\n",
        "- `ride_id:string`: Identificador √∫nico del viaje (texto).\n",
        "- `latitude:float`: Coordenada de latitud (decimal).\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- La tabla `realtime` almacenar√° los datos de los viajes de taxi en tiempo real, con particionamiento por tiempo para mejorar el rendimiento de las consultas.\n",
        "\n",
        "üí° **Contexto empresarial**: Crear una tabla es como definir un libro contable con columnas espec√≠ficas para registrar transacciones financieras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2.3: Crear un bucket en Cloud Storage\n",
        "\n",
        "1. Guarda el nombre del bucket como variable (reemplaza `your-project-id` con tu ID de proyecto):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "export BUCKET_NAME=\"your-project-id\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Crea el bucket:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gsutil mb gs://$BUCKET_NAME/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "- `gsutil`: Herramienta de l√≠nea de comandos para **Cloud Storage**.\n",
        "- `mb`: Comando para crear un bucket (\"make bucket\").\n",
        "- `gs://$BUCKET_NAME/`: Ruta del bucket usando la variable definida.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El bucket almacenar√° archivos temporales generados por el pipeline de **Dataflow**.\n",
        "\n",
        "üí° **Contexto empresarial**: Un bucket es como un archivo digital donde se guardan documentos financieros temporales durante el procesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñ±Ô∏è Tarea 3 (Alternativa): Crear todo usando Google Cloud Console\n",
        "\n",
        "Si prefieres usar la interfaz gr√°fica en lugar de comandos:\n",
        "\n",
        "### BigQuery\n",
        "1. Ve a **Navigation menu** > **BigQuery**.\n",
        "2. Haz clic en **‚ãÆ** junto al nombre del proyecto > **Create dataset**.\n",
        "3. Configura:\n",
        "   - **ID**: `taxirides`\n",
        "   - **Region**: `us`\n",
        "4. Crea la tabla `realtime`:\n",
        "   - Haz clic en el dataset `taxirides` > **Create table**.\n",
        "   - Usa la opci√≥n **Edit as text** para ingresar el esquema:\n",
        "     ```\n",
        "     ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer\n",
        "     ```\n",
        "   - Habilita **Time partitioning** > selecciona el campo `timestamp`.\n",
        "\n",
        "### Cloud Storage\n",
        "1. Ve a **Navigation menu** > **Storage** > **Buckets** > **Create bucket**.\n",
        "2. Configura:\n",
        "   - **Name**: Usa el mismo nombre que tu ID de proyecto.\n",
        "   - Mant√©n la configuraci√≥n por defecto.\n",
        "3. Haz clic en **Create**.\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- Esta alternativa es √∫til para usuarios que prefieren interfaces gr√°ficas sobre comandos.\n",
        "\n",
        "üí° **Contexto empresarial**: Usar la consola es como configurar un sistema contable mediante men√∫s en lugar de scripts, facilitando la adopci√≥n para equipos no t√©cnicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Tarea 4: Ejecutar el pipeline de Dataflow\n",
        "\n",
        "En esta tarea, ejecutar√°s un pipeline de streaming utilizando la plantilla **Pub/Sub to BigQuery**.\n",
        "\n",
        "1. En **Cloud Shell**, ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gcloud dataflow jobs run iotflow \\\n",
        "--gcs-location gs://dataflow-templates-\"Region\"/latest/PubSub_to_BigQuery \\\n",
        "--region \"Region\" \\\n",
        "--worker-machine-type e2-medium \\\n",
        "--staging-location gs://$BUCKET_NAME/temp \\\n",
        "--parameters inputTopic=projects/pubsub-public-data/topics/taxirides-realtime,outputTableSpec=\"PROJECT_ID:taxirides.realtime\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Desglose del comando**:\n",
        "| **Parte** | **Significado** |\n",
        "|-----------|-----------------|\n",
        "| `gcloud dataflow jobs run iotflow` | Crea un trabajo de **Dataflow** llamado `iotflow`. |\n",
        "| `--gcs-location ...` | Ubicaci√≥n de la plantilla p√∫blica **Pub/Sub to BigQuery**. Reemplaza `Region` con la regi√≥n de tu proyecto (por ejemplo, `us-central1`). |\n",
        "| `--region` | Regi√≥n donde se ejecutar√° el pipeline (por ejemplo, `us-central1`). |\n",
        "| `--worker-machine-type e2-medium` | Tipo de m√°quina para procesar los datos. |\n",
        "| `--staging-location` | Ubicaci√≥n temporal en el bucket para archivos intermedios. |\n",
        "| `--parameters` | Especifica el tema de **Pub/Sub** (`inputTopic`) y la tabla de **BigQuery** (`outputTableSpec`). Reemplaza `PROJECT_ID` con tu ID de proyecto. |\n",
        "\n",
        "**Explicaci√≥n**:\n",
        "- El pipeline lee datos de un tema p√∫blico de **Pub/Sub** (`taxirides-realtime`) y los inserta en la tabla `taxirides.realtime` en **BigQuery**.\n",
        "\n",
        "üí° **Contexto empresarial**: Este pipeline es como un sistema automatizado que registra transacciones financieras en tiempo real en un libro contable digital."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Tarea 5: Consultar los datos en BigQuery\n",
        "\n",
        "En esta tarea, verificar√°s los datos insertados por el pipeline.\n",
        "\n",
        "1. Ve a **BigQuery** en **Google Cloud Console**.\n",
        "2. Ejecuta la siguiente consulta SQL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SELECT * FROM `your-project-id.taxirides.realtime` LIMIT 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explicaci√≥n**:\n",
        "- Muestra los primeros 1000 registros de la tabla `realtime`, que contienen los datos de los viajes de taxi procesados por el pipeline.\n",
        "\n",
        "üí° **Contexto empresarial**: Esta consulta es como revisar un informe financiero para confirmar que las transacciones se han registrado correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ùì Tarea 6: Preguntas de repaso\n",
        "\n",
        "| **Pregunta** | **Respuesta** |\n",
        "|--------------|---------------|\n",
        "| ¬øGoogle Cloud Dataflow permite procesamiento por lotes? | ‚úÖ Verdadero |\n",
        "| ¬øCu√°l fue la plantilla usada en el laboratorio? | ‚úÖ Pub/Sub to BigQuery |\n",
        "\n",
        "## üöÄ Resumen\n",
        "\n",
        "| **Concepto** | **Explicaci√≥n contable simplificada** |\n",
        "|--------------|--------------------------------------|\n",
        "| **Dataflow** | Sistema automatizado que procesa transacciones financieras en tiempo real o por lotes. |\n",
        "| **Pub/Sub** | Cola de mensajes que recibe datos financieros en tiempo real, como transacciones. |\n",
        "| **BigQuery** | Base de datos anal√≠tica que almacena registros financieros para an√°lisis. |\n",
        "| **Pipeline** | Proceso automatizado que traslada datos financieros de una fuente a un destino. |\n",
        "\n",
        "üí° **Conclusi√≥n empresarial**: La plantilla **Pub/Sub to BigQuery** de **Dataflow** permite a las empresas procesar y almacenar datos en tiempo real, como registros de transacciones, de forma escalable y eficiente. Esto asegura an√°lisis r√°pidos, cumplimiento normativo y optimizaci√≥n de costos operativos.\n",
        "\n",
        "Para m√°s informaci√≥n, consulta la [Documentaci√≥n de Google Cloud Dataflow](https://cloud.google.com/dataflow/docs) y la [Documentaci√≥n de Google Cloud BigQuery](https://cloud.google.com/bigquery/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SQL",
      "language": "sql",
      "name": "sql"
    },
    "language_info": {
      "name": "sql"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}